{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ue5yM9PPb_vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f19d29-729f-4dff-b9a6-0910bdc0e48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.0)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.19.0\n",
            "    Uninstalling tensorflow-2.19.0:\n",
            "      Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.20.0 tensorflow-2.20.0\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, Any"
      ],
      "metadata": {
        "id": "_rrHtHXPioEx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4vCp4MyeO3c",
        "outputId": "6363c5f0-dcf7-4d3c-83b5-1de8ff71b50e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = '/content/drive/MyDrive/ANOMALY_DETECTION/'"
      ],
      "metadata": {
        "id": "f9nuNNgFdztb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "DRIVE_PATH = '/content/drive/MyDrive/ANOMALY_DETECTION/'\n",
        "original_data_path = drive_path + 'clean_data.csv'\n",
        "synthetic_data_path = drive_path + 'final_balanced_dataset.csv'\n",
        "artifacts_path = drive_path\n",
        "\n",
        "# Model & Preprocessing Parameters\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "PCA_COMPONENTS = 0.95\n",
        "\n",
        "def load_and_prepare_data(original_path: str, synthetic_path: str, test_size: float, random_state: int) -> Dict[str, Any]:\n",
        "    \"\"\"Loads, splits, and combines original and synthetic data.\"\"\"\n",
        "    print(\"--- Step 1: Loading and Preparing Data ---\")\n",
        "    original_df = pd.read_csv(original_path)\n",
        "    synthetic_df = pd.read_csv(synthetic_path)\n",
        "\n",
        "    # Create a stratified hold-out test set from the original data\n",
        "    train_df, test_df = train_test_split(original_df, test_size=test_size, random_state=random_state, stratify=original_df['Label'])\n",
        "    joblib.dump(test_df, artifacts_path + 'hold_out_test_set.pkl')\n",
        "    print(f\"Hold-out test set created and saved. Shape: {test_df.shape}\")\n",
        "\n",
        "    # Create the full training set by combining original training data with synthetic attacks\n",
        "    synthetic_attacks_df = synthetic_df[synthetic_df['Label'] != 'BENIGN']\n",
        "    full_train_df = pd.concat([train_df, synthetic_attacks_df], ignore_index=True).sample(frac=1, random_state=random_state)\n",
        "    print(f\"Full training dataset assembled. Shape: {full_train_df.shape}\")\n",
        "\n",
        "    return {'train': full_train_df, 'test': test_df}\n",
        "\n",
        "def preprocess_data(train_df: pd.DataFrame, test_df: pd.DataFrame, pca_components: float) -> Dict[str, Any]:\n",
        "    \"\"\"Applies a full preprocessing pipeline to the data.\"\"\"\n",
        "    print(\"--- Step 2: Preprocessing Data ---\")\n",
        "    # One-hot encode categorical features\n",
        "    categorical_cols = ['Destination_port_group']\n",
        "    train_encoded = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)\n",
        "    test_encoded = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "    # Align columns to ensure consistency\n",
        "    train_labels = train_encoded['Label']\n",
        "    train_features = train_encoded.drop(columns=['Label'])\n",
        "    test_labels = test_encoded['Label']\n",
        "    test_features = test_encoded.drop(columns=['Label'])\n",
        "    train_features, test_features = train_features.align(test_features, join='left', axis=1, fill_value=0)\n",
        "\n",
        "    # Isolate benign training data to fit the preprocessors\n",
        "    benign_train_features = train_features[train_labels == 'BENIGN']\n",
        "    print(f\"Fitting Scaler and PCA on {len(benign_train_features)} benign samples...\")\n",
        "\n",
        "    # Fit Scaler and PCA ONLY on benign training data to prevent data leakage\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(benign_train_features)\n",
        "    pca = PCA(n_components=pca_components, random_state=RANDOM_STATE)\n",
        "    pca.fit(scaler.transform(benign_train_features))\n",
        "\n",
        "    # Transform all relevant data subsets\n",
        "    benign_train_pca = pca.transform(scaler.transform(benign_train_features))\n",
        "\n",
        "    attack_train_features = train_features[train_labels != 'BENIGN']\n",
        "    attack_train_labels = train_labels[train_labels != 'BENIGN']\n",
        "    attack_train_pca = pca.transform(scaler.transform(attack_train_features))\n",
        "\n",
        "    return {\n",
        "        'benign_train_pca': benign_train_pca,\n",
        "        'attack_train_pca': attack_train_pca,\n",
        "        'attack_train_labels': attack_train_labels,\n",
        "        'scaler': scaler,\n",
        "        'pca': pca\n",
        "    }\n",
        "\n",
        "def train_autoencoder(data: np.ndarray) -> (Model, float):\n",
        "    \"\"\"Trains an Autoencoder for anomaly detection.\"\"\"\n",
        "    print(\"--- Step 3: Training Autoencoder ---\")\n",
        "    X_train, X_val = train_test_split(data, test_size=0.2, random_state=RANDOM_STATE)\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    encoding_dim1 = int(input_dim * 0.75)\n",
        "    encoding_dim2 = int(input_dim * 0.5)\n",
        "\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(encoding_dim1, activation=\"relu\")(input_layer)\n",
        "    encoder = Dense(encoding_dim2, activation=\"relu\")(encoder)\n",
        "    decoder = Dense(encoding_dim1, activation=\"relu\")(encoder)\n",
        "    decoder = Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss'),\n",
        "        ReduceLROnPlateau(patience=3, factor=0.5, monitor='val_loss')\n",
        "    ]\n",
        "\n",
        "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, validation_data=(X_val, X_val), callbacks=callbacks, verbose=1)\n",
        "\n",
        "    # Determine anomaly threshold\n",
        "    reconstructions = autoencoder.predict(X_val)\n",
        "    errors = np.mean(np.square(X_val - reconstructions), axis=1)\n",
        "    threshold = np.mean(errors) + 3 * np.std(errors)\n",
        "    print(f\"Autoencoder Anomaly Threshold set to: {threshold}\")\n",
        "\n",
        "    return autoencoder, threshold\n",
        "\n",
        "def train_classifier(features: np.ndarray, labels: pd.Series) -> (RandomForestClassifier, LabelEncoder):\n",
        "    \"\"\"Trains a RandomForest classifier for attack classification.\"\"\"\n",
        "    print(\"--- Step 4: Training RandomForest Classifier ---\")\n",
        "    label_encoder = LabelEncoder()\n",
        "    encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced')\n",
        "    rf_classifier.fit(features, encoded_labels)\n",
        "\n",
        "    return rf_classifier, label_encoder\n",
        "\n",
        "def save_artifacts(path: str, artifacts: Dict[str, Any]):\n",
        "    \"\"\"Saves all model and preprocessor artifacts.\"\"\"\n",
        "    print(\"--- Step 5: Saving All Artifacts ---\")\n",
        "    for name, artifact in artifacts.items():\n",
        "        if name == 'autoencoder':\n",
        "            artifact.save(path + f'{name}.h5')\n",
        "        else:\n",
        "            joblib.dump(artifact, path + f'{name}.pkl')\n",
        "    print(\"✅ All artifacts saved successfully.\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the entire training pipeline.\"\"\"\n",
        "    # Step 1: Load and split data\n",
        "    data_dict = load_and_prepare_data(original_data_path, synthetic_data_path, TEST_SIZE, RANDOM_STATE)\n",
        "\n",
        "    # Step 2: Preprocess data and get components for training\n",
        "    preprocessed_dict = preprocess_data(data_dict['train'], data_dict['test'], PCA_COMPONENTS)\n",
        "\n",
        "    # Step 3: Train the anomaly detection model\n",
        "    autoencoder, threshold = train_autoencoder(preprocessed_dict['benign_train_pca'])\n",
        "\n",
        "    # Step 4: Train the attack classification model\n",
        "    rf_classifier, label_encoder = train_classifier(preprocessed_dict['attack_train_pca'], preprocessed_dict['attack_train_labels'])\n",
        "\n",
        "    # Step 5: Save all the generated artifacts\n",
        "    artifacts_to_save = {\n",
        "        'scaler': preprocessed_dict['scaler'],\n",
        "        'pca_unified': preprocessed_dict['pca'],\n",
        "        'autoencoder': autoencoder,\n",
        "        'threshold': threshold,\n",
        "        'rf_classifier': rf_classifier,\n",
        "        'label_encoder': label_encoder,\n",
        "        'training_columns': preprocessed_dict['scaler'].feature_names_in_ # Save column names for future inference\n",
        "    }\n",
        "    save_artifacts(artifacts_path, artifacts_to_save)\n",
        "    print(\"\\nTraining pipeline complete. You can now evaluate performance on 'hold_out_test_set.pkl'.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfkB5GBoiZsx",
        "outputId": "3298da3b-01fa-422b-cedf-0eda6ac8e6a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Loading and Preparing Data ---\n",
            "Hold-out test set created and saved. Shape: (565569, 78)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJ1aQZ3Vdptf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}